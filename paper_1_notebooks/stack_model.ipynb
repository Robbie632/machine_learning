{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data_formodel\n",
    "%store -r test_data\n",
    "%store -r my_data\n",
    "%store -r uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_formodel[train_data_formodel.columns.values[9:,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232', 'U238', 'class'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['class'])\n",
    "train_data_feats = train_data.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train = np.array(my_scaler.fit_transform(train_data_feats))\n",
    "x_train = np.array(train_data_feats)\n",
    "x_test = np.array(test_data[test_data.columns.values[9:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1243, 53)\n",
      "(363, 53)\n",
      "(1243,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]\n",
    "SEED = 42 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, set_seed = True):\n",
    "\n",
    "        if set_seed:\n",
    "            params['random_state'] = seed\n",
    "\n",
    "            \n",
    "        self.clf = clf(**params)\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 50, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':50,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 50,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "svc_params = {\n",
    "    'C' : 2, 'kernel':'linear', 'verbose':3\n",
    "    }\n",
    "# Support Vector Classifier parameters \n",
    "\n",
    "\n",
    "knn_params = {\n",
    "    'n_jobs' : -1\n",
    "}\n",
    "\n",
    "bgg_params = {\n",
    "    'n_jobs' : -1,\n",
    "    'n_estimators':50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n",
    "knn = SklearnHelper(clf=KNeighborsClassifier, params=knn_params, set_seed = False)\n",
    "bgg = SklearnHelper(clf=BaggingClassifier, seed=SEED, params=bgg_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting random forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting adaboost\n",
      "starting gradient boost\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1737.0093           10.03s\n",
      "         2        1386.1203           10.10s\n",
      "         3        1147.8542           10.01s\n",
      "         4         963.7001            9.84s\n",
      "         5         816.1994            9.69s\n",
      "         6         692.1486            9.49s\n",
      "         7         594.9344            9.27s\n",
      "         8         511.3994            9.10s\n",
      "         9         442.3408            8.90s\n",
      "        10         382.6964            8.70s\n",
      "        20         101.0008            6.79s\n",
      "        30          35.4033            4.53s\n",
      "        40          16.9764            2.25s\n",
      "        50          11.4030            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1716.0056            8.67s\n",
      "         2        1358.0704            8.60s\n",
      "         3        1112.8111            8.54s\n",
      "         4         933.5551            8.39s\n",
      "         5         789.7129            8.25s\n",
      "         6         670.1964            8.15s\n",
      "         7         576.6177            8.02s\n",
      "         8         497.0750            7.87s\n",
      "         9         429.3939            7.76s\n",
      "        10         370.4865            7.58s\n",
      "        20         103.3654            5.81s\n",
      "        30          40.0031            3.91s\n",
      "        40          22.6693            1.93s\n",
      "        50          17.1172            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1704.4438           10.10s\n",
      "         2        1357.2826           10.81s\n",
      "         3        1114.7265           10.47s\n",
      "         4         929.4645           10.10s\n",
      "         5         788.9825            9.81s\n",
      "         6         673.1687            9.72s\n",
      "         7         577.1692            9.55s\n",
      "         8         494.9933            9.29s\n",
      "         9         425.1518            9.08s\n",
      "        10         366.9941            8.89s\n",
      "        20          99.5246            6.76s\n",
      "        30          37.8052            4.52s\n",
      "        40          20.5180            2.22s\n",
      "        50          15.1662            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1658.6473            9.90s\n",
      "         2        1311.0868           10.15s\n",
      "         3        1075.0542           10.12s\n",
      "         4         897.7734            9.99s\n",
      "         5         749.0691            9.81s\n",
      "         6         631.3275            9.86s\n",
      "         7         534.1702           10.01s\n",
      "         8         451.2533            9.73s\n",
      "         9         383.5299            9.63s\n",
      "        10         324.7660            9.49s\n",
      "        20          78.8680            7.03s\n",
      "        30          26.5031            4.61s\n",
      "        40          13.9469            2.21s\n",
      "        50          10.7801            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1643.5945           11.65s\n",
      "         2        1319.3011           11.59s\n",
      "         3        1085.6242           11.44s\n",
      "         4         910.4726           11.81s\n",
      "         5         767.8606           11.47s\n",
      "         6         650.6031           11.27s\n",
      "         7         554.1066           11.33s\n",
      "         8         472.0350           11.03s\n",
      "         9         401.5857           10.96s\n",
      "        10         342.2894           10.83s\n",
      "        20          87.1453            8.07s\n",
      "        30          33.9759            5.47s\n",
      "        40          21.5625            2.64s\n",
      "        50          18.6654            0.00s\n",
      "starting support vector machine\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "print('starting random forest')\n",
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "print('starting adaboost')\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
    "print('starting gradient boost')\n",
    "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
    "print('starting support vector machine')\n",
    "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n",
    "print('starting knn')\n",
    "knn_oof_train, knn_oof_test = get_oof(knn,x_train, y_train, x_test)\n",
    "print('starting bagging')\n",
    "bgg_oof_train, bgg_oof_test = get_oof(bgg,x_train, y_train, x_test)\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame( \n",
    "    {'RandomForest': rf_oof_train.ravel(),\n",
    "    'ExtraTrees': et_oof_train.ravel(),\n",
    "     'AdaBoost': ada_oof_train.ravel(),\n",
    "      'GradientBoost': gb_oof_train.ravel(),\n",
    "    'supportvector': svc_oof_train.ravel(),\n",
    "    'knn': knn_oof_train.ravel(),\n",
    "    'bgg': bgg_oof_train.ravel()\n",
    "})\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_test = pd.DataFrame( \n",
    "    {'RandomForest': rf_oof_test.ravel(),\n",
    "     'ExtraTrees': et_oof_test.ravel(),\n",
    "     'AdaBoost': ada_oof_test.ravel(),\n",
    "      'GradientBoost': gb_oof_test.ravel(),\n",
    "    'supportvector': svc_oof_test.ravel(),\n",
    "    'knn': knn_oof_test.ravel(),\n",
    "     'bgg': bgg_oof_test.ravel()                                  \n",
    "    })\n",
    "base_predictions_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_train = np.array(base_predictions_train)\n",
    "y_2_train = np.array(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train.astype(float).corr().values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x=base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Rainbow',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace]\n",
    "fig = dict(data=data)\n",
    "\n",
    "py.plot(fig, output_type='file', filename='corrheatmap'+ '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(data = base_predictions_train.astype(float).corr().values, columns = base_predictions_train.columns.values)\n",
    "corr_df_abs = corr_df.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summations = []\n",
    "for col in corr_df_abs.columns.values:\n",
    "    summations.append(corr_df_abs[col].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_correlation_summations = pd.DataFrame(data = summations, index=list(corr_df_abs.columns.values))\n",
    "model_correlation_summations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(max_depth = 20, n_estimators = 2000, n_jobs=-1)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X_2_train, y_2_train)\n",
    "class_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "accuracy_scores = []\n",
    "feat_imp =[]\n",
    "f1_dict = {}\n",
    "count = 0\n",
    "for train_index, test_index in skf.split(X_2_train, y_2_train):\n",
    "    count = count + 1\n",
    "    print('making model:')\n",
    "    key = 'round' + str(count)\n",
    "    print(count)\n",
    "    X_train_loc, X_test_loc = X_2_train[train_index], X_2_train[test_index]\n",
    "    y_train_loc, y_test_loc = y_2_train[train_index], y_2_train[test_index]\n",
    "    xg.fit(X_train_loc, y_train_loc)\n",
    "    y_pred = xg.predict(X_test_loc)\n",
    "    class_f1_scores = f1_score(y_test_loc, y_pred, average = None)\n",
    "    accuracy = accuracy_score(y_test_loc, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    macro_f1_scores.append(f1_score(y_test_loc, y_pred, average = 'weighted'))\n",
    "    f1_dict[key] = class_f1_scores \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = pd.DataFrame(data = f1_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f1_dict:\n",
    "    print(len(f1_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the encodings for the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['class'].unique())\n",
    "print(list(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final = pd.concat([f1_df, pd.Series(uniques)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final.rename(columns={0:'class'}, inplace=True)\n",
    "f1_df_final.set_index('class', drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the distribution of class f1 scores from 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plot = sns.boxplot(data = f1_df_final.T)\n",
    "plot.set_title('F1 scores for each site', fontdict={'fontsize': 14})\n",
    "plot.set_ylabel('F1 score', fontdict={'fontsize': 11})\n",
    "plot.set_xlabel(\"Bedrock site or superficial deposit 'region'\", fontdict={'fontsize': 11})\n",
    "\n",
    "#f save_plots == True:\n",
    "#    fig = plot.get_figure()\n",
    "#    fig.savefig('site_specific_f1_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the macro F1 score with weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.boxplot(macro_f1_scores)\n",
    "plot.set_title('Average-weighted macro-f1 score', fontdict={'fontsize': 14})\n",
    "plot.set_xlabel(\"F1-score\", fontdict={'fontsize': 11})\n",
    "\n",
    "#if save_plots == True:\n",
    "#    fig = plot.get_figure()\n",
    "#    fig.savefig('macro_f1_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
