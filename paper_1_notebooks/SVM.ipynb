{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load variables stored by data_preproccessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data\n",
    "%store -r test_data\n",
    "%store -r my_data\n",
    "%store -r uniques\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232', 'U238'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn feature data and class to be predicited into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_data[train_data.columns.values[9:-1]])\n",
    "y = np.array(train_data['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carry out 10-f0ld stratified cross validation, class f1 scores and macro f1 scores with weighted averages are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making model:\n",
      "1\n",
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "svc = SVC(verbose=3)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X, y)\n",
    "class_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "accuracy_scores = []\n",
    "feat_imp =[]\n",
    "f1_dict = {}\n",
    "count = 0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    count = count + 1\n",
    "    print('making model:')\n",
    "    key = 'round' + str(count)\n",
    "    print(count)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    svc.fit(X_train, y_train)\n",
    "    y_pred = svc.predict(X_test)\n",
    "    class_f1_scores = f1_score(y_test, y_pred, average = None)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    macro_f1_scores.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "    f1_dict[key] = class_f1_scores \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = pd.DataFrame(data = f1_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f1_dict:\n",
    "    print(len(f1_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the encodings for the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['class'].unique())\n",
    "print(list(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final = pd.concat([f1_df, pd.Series(uniques)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final.rename(columns={0:'class'}, inplace=True)\n",
    "f1_df_final.set_index('class', drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the distribution of class f1 scores from 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plot = sns.boxplot(data = f1_df_final.T)\n",
    "plot.set_title('F1 scores for each site', fontdict={'fontsize': 14})\n",
    "plot.set_ylabel('F1 score', fontdict={'fontsize': 11})\n",
    "plot.set_xlabel(\"Bedrock site or superficial deposit 'region'\", fontdict={'fontsize': 11})\n",
    "\n",
    "if save_plots == True:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('site_specific_f1_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the macro F1 score with weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.boxplot(macro_f1_scores)\n",
    "plot.set_title('Average-weighted macro-f1 score', fontdict={'fontsize': 14})\n",
    "plot.set_xlabel(\"F1-score\", fontdict={'fontsize': 11})\n",
    "\n",
    "if save_plots == True:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('macro_f1_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df = pd.DataFrame(data = feat_imp_dict)\n",
    "feat_imp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final = pd.concat([feat_imp_df, pd.Series(my_data[my_data.columns.values[9:-1]].columns.values)], axis = 1)\n",
    "feat_imp_df_final.rename(columns = {0:'element'}, inplace = True )\n",
    "feat_imp_df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final.set_index('element', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final_plot = feat_imp_df_final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp_df_final_plot\n",
    "\n",
    "elements = feat_imp_df_final_plot.columns.values \n",
    "mean_feature_importance = []\n",
    "for col in list(feat_imp_df_final_plot.columns.values):\n",
    "    mean_feature_importance.append(feat_imp_df_final_plot[col].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_feature_importance_df = pd.concat([pd.Series(elements), pd.Series(mean_feature_importance)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_feature_importance_df.rename(columns={0:'elements', 1:'mean_importance'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_feature_importance_df.sort_values(by='mean_importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_col_names = list(mean_feature_importance_df['elements'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style()\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "plot = sns.boxplot(data = feat_imp_df_final_plot[ordered_col_names])\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation=90, ha = 'left')\n",
    "plot.set_title('Feature (element) importance', fontdict={'fontsize': 20})\n",
    "plot.set_ylabel('Feature importance', fontdict={'fontsize': 15})\n",
    "plot.set_xlabel(\"Element\", fontdict={'fontsize': 15})\n",
    "\n",
    "if save_plots == True:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('feature_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model is built for predicting source of artefacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_final = RandomForestClassifier(n_estimators=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFC_final.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['class'].unique())\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_identifiers = test_data.copy(deep = True)\n",
    "identifiers =  df_for_identifiers['Analysis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions are made for the artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = RFC_final.predict(np.array(test_data[test_data.columns.values[8:-1]]))\n",
    "\n",
    "y_pred_proba = RFC_final.predict_proba(np.array(test_data[test_data.columns.values[8:-1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_df = pd.DataFrame(data = y_pred_proba, columns = uniques)\n",
    "probabilities_df_final = pd.concat([probabilities_df, pd.Series(list(identifiers))], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities_df_final.rename(columns = {0:'identifier'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df = pd.concat([pd.Series(y_pred), probabilities_df_final], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df.rename(columns={0:'class_number'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df.to_csv('predictions.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniques_list = list(uniques)\n",
    "def get_pred_names(row):\n",
    "    return(uniques_list[row['class_number']])\n",
    "final_pred_df['class_predictions'] = final_pred_df.apply(get_pred_names, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_pred_df_modal = final_pred_df.groupby(by = 'class_number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
