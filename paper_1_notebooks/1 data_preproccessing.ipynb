{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preproccessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configurations\n",
    "* classify_bedrock_only -> True|False, if set to True then classes for classification are bedrock sites only\n",
    "* group_superficial -> True|False, if set to True then some superficial sites are grouped together to raise class-specific F1 scores for the associated sites\n",
    "* drop_fake_bedrock ->  True|False, if set to True then some bedrock sites deemed not to be true bedrock sites are not used for classification\n",
    "* export_dimensions ->  True|False, if set to True then exports t-SNE and PCA dimensions as .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_path = '../data/raw_data.csv'\n",
    "\n",
    "group_superficial = True\n",
    "drop_fake_bedrock = True\n",
    "classify_bedrock_only = False\n",
    "reduced_sites = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(data_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FH', 'ER', 'WW', 'TC', 'CS', 'BC', 'KQ', 'AR', 'SL', 'FG', 'WB',\n",
       "       'BX', 'PF', 'BM', 'WH', 'SQ', 'BP', 'WN', 'BH', 'PH', 'LB', 'AB',\n",
       "       'LV', 'BR', 'KY', 'BF', 'ST', 'SH', 'CF', 'BG', 'AC', 'CR', 'GH',\n",
       "       'PX', 'WF', 'DH', 'NMAG_Gold', 'NMW_Gold', 'NMWGwern', 'UBSS',\n",
       "       'Cefn', 'Stockley', 'Pucha', 'Woodbury', 'Pimple', 'Wellington',\n",
       "       'Lyonshall', 'SymondsYatE', 'Madawg', nan], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['Site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_classes_grouped(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        elif row['Site'] == 'BC' or row['Site'] == 'CS':\n",
    "            return('BC_CS')\n",
    "        elif row['Site'] == 'SQ' or row['Site'] == 'BP':\n",
    "            return('SQ_BP')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "        \n",
    "def make_classes_grouped_reduced(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "\n",
    "def make_classes_raw(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        return(row['Region'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['class'] = 'init'   \n",
    "\n",
    "\n",
    "if drop_fake_bedrock and not group_superficial:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "    \n",
    "if group_superficial and not drop_fake_bedrock:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "else:\n",
    "    my_data['class'] = my_data.apply(make_classes_raw, axis = 1)\n",
    "if reduced_sites == True:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped_reduced, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_fake_bedrock:\n",
    "    my_data = my_data[(my_data['class'] != 'BM') & (my_data['class'] != 'BC') & (my_data['class'] != 'BP') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove '<' signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24103.77it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21796.11it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21340.30it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21739.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21773.14it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24089.23it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24433.82it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24055.76it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24417.14it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23928.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23360.73it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22218.37it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22606.64it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24487.86it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23540.16it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24436.84it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23182.03it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21618.26it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22894.06it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23332.84it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24011.48it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21459.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 18889.97it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23926.75it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22125.41it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21460.96it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24317.17it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22169.26it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22730.74it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21244.19it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21674.98it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22714.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23528.08it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23408.10it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 20257.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23916.99it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22410.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22706.24it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21214.83it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22013.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23995.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22078.13it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21878.59it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23656.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22914.61it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21595.16it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23053.71it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 21584.49it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22764.28it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22815.98it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24377.36it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22157.26it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23425.20it/s]\n"
     ]
    }
   ],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    def fill_less_than(row):\n",
    "        if 'DL' in  str(row[column_name]):\n",
    "            return(np.nan)\n",
    "        if '<' in str(row[column_name]):\n",
    "            return(float(row[column_name].replace('<', '').replace(',', '')))\n",
    "        else:\n",
    "            return(float(row[column_name]))\n",
    "    my_data[column_name] = my_data.swifter.apply(fill_less_than, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_data = my_data.dropna(thresh = 15 , axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute na values with variable mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    my_data[column_name] = my_data[column_name].fillna(my_data[column_name].mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers defined as any values that exceed 2 standard deviations from the mean, such values are changed to the mean for that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dict = {}\n",
    "mean_dict = {}\n",
    "median_dict = {}\n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    std_dict[col] = my_data[col].std()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    mean_dict[col] = my_data[col].mean()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    median_dict[col] = my_data[col].median()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 25621.14it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 25410.30it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26526.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 25790.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 21932.53it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26525.15it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27672.63it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26936.51it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28519.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 24693.58it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26384.29it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28119.29it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27309.31it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27697.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27534.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26946.41it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 23180.53it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 23354.92it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27307.20it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 24515.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 21219.99it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 24353.84it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26436.77it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27285.01it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27697.50it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28169.58it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27153.78it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28522.22it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27504.28it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28150.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27447.72it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26350.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27244.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26869.58it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27662.79it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 23893.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 25191.66it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 23652.09it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 25989.91it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 22853.44it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 23118.53it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27231.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 24058.20it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 24157.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28307.57it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26625.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26941.06it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 25910.01it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26349.63it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26740.17it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 23013.79it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 26599.36it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27286.88it/s]\n"
     ]
    }
   ],
   "source": [
    "for col_name in my_data.columns.values[9:-1]:\n",
    "    def impute_outliers(row):\n",
    "        if np.abs(row[col_name] - mean_dict[col_name]) > 2*(std_dict[col_name]):\n",
    "            return(mean_dict[col_name])\n",
    "        else:\n",
    "            return(row[col_name])\n",
    "    my_data[col_name]= my_data.swifter.apply(impute_outliers, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for samples for which the source is known for training the model and those for which the source is not known (artefacts) for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = my_data[(my_data['Geology']== 'Bedrock') | (my_data['Geology'] == 'Superficial')]\n",
    "test_data = my_data[my_data['Geology']=='Artefacts']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encode the class to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_formodel = train_data.copy(deep = True)\n",
    "if classify_bedrock_only == False: \n",
    "    train_data_formodel['class'], uniques = pd.factorize(train_data_formodel['class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_formodel' (DataFrame)\n",
      "Stored 'train_data' (DataFrame)\n",
      "Stored 'test_data' (DataFrame)\n",
      "Stored 'my_data' (DataFrame)\n",
      "Stored 'uniques' (Index)\n"
     ]
    }
   ],
   "source": [
    "%store train_data_formodel\n",
    "%store train_data\n",
    "%store test_data\n",
    "%store my_data\n",
    "%store uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data is split into two datasets, one superficial and one bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bedrock = train_data[train_data['Geology'] == 'Bedrock']\n",
    "train_data_superficial = train_data[train_data['Geology'] == 'Superficial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the columns containimg the mass spectrometry data are stored as variables, 4 datsets are stored, one containing all train data (bedrock and superficial), one containing just bedrock, one containing just superficial and one containing the artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232', 'U238'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_bedrock.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train = train_data[train_data.columns.values[9:-1]]\n",
    "element_data_train_bedrock = train_data_bedrock[train_data.columns.values[9:-1]]\n",
    "element_data_train_superficial = train_data_superficial[train_data.columns.values[9:-1]]\n",
    "element_data_test = test_data[test_data.columns.values[9:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I scale the mass spectrometry data to have a mean 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler_train = StandardScaler()\n",
    "my_scaler_train_bedrock = StandardScaler()\n",
    "my_scaler_train_superficial = StandardScaler()\n",
    "my_scaler_test = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train_scaled = my_scaler_train.fit_transform(element_data_train)\n",
    "element_data_train_bedrock_scaled = my_scaler_train_bedrock.fit_transform(element_data_train_bedrock)\n",
    "element_data_train_superficial_scaled = my_scaler_train_superficial.fit_transform(element_data_train_superficial)\n",
    "element_data_test_scaled = my_scaler_test.fit_transform(element_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA is utilised on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca_train = PCA(n_components=element_data_train_scaled.shape[1])\n",
    "\n",
    "my_pca_train_bedrock = PCA(n_components=element_data_train_bedrock_scaled.shape[1])\n",
    "my_pca_train_superficial = PCA(n_components=element_data_train_superficial_scaled.shape[1])\n",
    "\n",
    "my_pca_test = PCA(n_components=element_data_test_scaled.shape[1])\n",
    "\n",
    "element_data_train_pca = my_pca_train.fit_transform(element_data_train_scaled)\n",
    "\n",
    "element_data_train_bedrock_pca = my_pca_train_bedrock.fit_transform(element_data_train_bedrock_scaled)\n",
    "element_data_train_superficial_pca = my_pca_train_superficial.fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "element_data_test_pca = my_pca_test.fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the principal components for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_PCs = element_data_train_scaled.shape[1]\n",
    "PC_names = []\n",
    "for i in range(0, no_PCs):\n",
    "    number = i + 1\n",
    "    column_name = 'PC' + str(number)\n",
    "    PC_names.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_df_train = pd.DataFrame(data = element_data_train_pca, columns = PC_names)\n",
    "\n",
    "PC_df_bedrock_train = pd.DataFrame(data = element_data_train_bedrock_pca, columns = PC_names)\n",
    "PC_df_superficial_train = pd.DataFrame(data = element_data_train_superficial_pca, columns = PC_names)\n",
    "\n",
    "PC_df_test = pd.DataFrame(data = element_data_test_pca, columns = PC_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE is utilised on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 1158 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 1158 samples in 0.187s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1158\n",
      "[t-SNE] Computed conditional probabilities for sample 1158 / 1158\n",
      "[t-SNE] Mean sigma: 2.027810\n",
      "[t-SNE] Computed conditional probabilities in 0.071s\n",
      "[t-SNE] Iteration 50: error = 72.9022751, gradient norm = 0.1194551 (50 iterations in 5.077s)\n",
      "[t-SNE] Iteration 100: error = 73.8381500, gradient norm = 0.0987646 (50 iterations in 3.793s)\n",
      "[t-SNE] Iteration 150: error = 74.3681564, gradient norm = 0.1034126 (50 iterations in 2.729s)\n",
      "[t-SNE] Iteration 200: error = 74.3389893, gradient norm = 0.0824283 (50 iterations in 2.519s)\n",
      "[t-SNE] Iteration 250: error = 74.7075882, gradient norm = 0.0842570 (50 iterations in 2.994s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 74.707588\n",
      "[t-SNE] Iteration 300: error = 1.5202278, gradient norm = 0.0010219 (50 iterations in 2.766s)\n",
      "[t-SNE] Iteration 350: error = 1.3953396, gradient norm = 0.0002070 (50 iterations in 3.395s)\n",
      "[t-SNE] Iteration 400: error = 1.3014903, gradient norm = 0.0003460 (50 iterations in 3.632s)\n",
      "[t-SNE] Iteration 450: error = 1.2801124, gradient norm = 0.0001062 (50 iterations in 3.650s)\n",
      "[t-SNE] Iteration 500: error = 1.2664762, gradient norm = 0.0000815 (50 iterations in 3.804s)\n",
      "[t-SNE] Iteration 550: error = 1.2525904, gradient norm = 0.0000561 (50 iterations in 3.805s)\n",
      "[t-SNE] Iteration 600: error = 1.2332414, gradient norm = 0.0000518 (50 iterations in 3.978s)\n",
      "[t-SNE] Iteration 650: error = 1.1983898, gradient norm = 0.0001401 (50 iterations in 4.394s)\n",
      "[t-SNE] Iteration 700: error = 1.1779249, gradient norm = 0.0000558 (50 iterations in 4.188s)\n",
      "[t-SNE] Iteration 750: error = 1.1622360, gradient norm = 0.0000463 (50 iterations in 4.152s)\n",
      "[t-SNE] KL divergence after 750 iterations: 1.162236\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 726 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 726 samples in 0.064s...\n",
      "[t-SNE] Computed conditional probabilities for sample 726 / 726\n",
      "[t-SNE] Mean sigma: 2.151001\n",
      "[t-SNE] Computed conditional probabilities in 0.036s\n",
      "[t-SNE] Iteration 50: error = 78.9555435, gradient norm = 0.2601515 (50 iterations in 1.732s)\n",
      "[t-SNE] Iteration 100: error = 85.3247299, gradient norm = 0.2232216 (50 iterations in 1.819s)\n",
      "[t-SNE] Iteration 150: error = 89.0337753, gradient norm = 0.2204180 (50 iterations in 1.577s)\n",
      "[t-SNE] Iteration 200: error = 92.2407227, gradient norm = 0.1905511 (50 iterations in 1.514s)\n",
      "[t-SNE] Iteration 250: error = 92.6395493, gradient norm = 0.1846970 (50 iterations in 1.862s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 92.639549\n",
      "[t-SNE] Iteration 300: error = 2.6788700, gradient norm = 0.0007220 (50 iterations in 1.710s)\n",
      "[t-SNE] Iteration 350: error = 2.3230286, gradient norm = 0.0003089 (50 iterations in 1.879s)\n",
      "[t-SNE] Iteration 400: error = 2.1214364, gradient norm = 0.0002695 (50 iterations in 1.944s)\n",
      "[t-SNE] Iteration 450: error = 1.9931589, gradient norm = 0.0001134 (50 iterations in 1.850s)\n",
      "[t-SNE] Iteration 500: error = 1.8949262, gradient norm = 0.0000809 (50 iterations in 1.848s)\n",
      "[t-SNE] Iteration 550: error = 1.8162875, gradient norm = 0.0000776 (50 iterations in 1.892s)\n",
      "[t-SNE] Iteration 600: error = 1.7390218, gradient norm = 0.0000616 (50 iterations in 1.935s)\n",
      "[t-SNE] Iteration 650: error = 1.6844331, gradient norm = 0.0000496 (50 iterations in 1.823s)\n",
      "[t-SNE] Iteration 700: error = 1.6369416, gradient norm = 0.0000401 (50 iterations in 1.751s)\n",
      "[t-SNE] Iteration 750: error = 1.5930864, gradient norm = 0.0000379 (50 iterations in 1.745s)\n",
      "[t-SNE] KL divergence after 750 iterations: 1.593086\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 432 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 432 samples in 0.023s...\n",
      "[t-SNE] Computed conditional probabilities for sample 432 / 432\n",
      "[t-SNE] Mean sigma: 2.304483\n",
      "[t-SNE] Computed conditional probabilities in 0.022s\n",
      "[t-SNE] Iteration 50: error = 89.3000183, gradient norm = 0.2794216 (50 iterations in 0.742s)\n",
      "[t-SNE] Iteration 100: error = 104.8208771, gradient norm = 0.2205808 (50 iterations in 0.683s)\n",
      "[t-SNE] Iteration 150: error = 116.3723831, gradient norm = 0.2199322 (50 iterations in 0.646s)\n",
      "[t-SNE] Iteration 200: error = 122.1802521, gradient norm = 0.1851525 (50 iterations in 0.698s)\n",
      "[t-SNE] Iteration 250: error = 126.3680420, gradient norm = 0.1765773 (50 iterations in 0.608s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 126.368042\n",
      "[t-SNE] Iteration 300: error = 3.7956500, gradient norm = 0.0005805 (50 iterations in 0.674s)\n",
      "[t-SNE] Iteration 350: error = 3.2208738, gradient norm = 0.0002336 (50 iterations in 0.691s)\n",
      "[t-SNE] Iteration 400: error = 2.9459205, gradient norm = 0.0001383 (50 iterations in 0.707s)\n",
      "[t-SNE] Iteration 450: error = 2.7729399, gradient norm = 0.0000895 (50 iterations in 0.664s)\n",
      "[t-SNE] Iteration 500: error = 2.6510220, gradient norm = 0.0000660 (50 iterations in 0.682s)\n",
      "[t-SNE] Iteration 550: error = 2.5511062, gradient norm = 0.0000540 (50 iterations in 0.685s)\n",
      "[t-SNE] Iteration 600: error = 2.4644175, gradient norm = 0.0000482 (50 iterations in 0.668s)\n",
      "[t-SNE] Iteration 650: error = 2.3913748, gradient norm = 0.0000373 (50 iterations in 0.715s)\n",
      "[t-SNE] Iteration 700: error = 2.3359690, gradient norm = 0.0000314 (50 iterations in 0.721s)\n",
      "[t-SNE] Iteration 750: error = 2.2857633, gradient norm = 0.0000266 (50 iterations in 0.672s)\n",
      "[t-SNE] KL divergence after 750 iterations: 2.285763\n",
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 363 samples in 0.001s...\n",
      "[t-SNE] Computed neighbors for 363 samples in 0.019s...\n",
      "[t-SNE] Computed conditional probabilities for sample 363 / 363\n",
      "[t-SNE] Mean sigma: 2.408957\n",
      "[t-SNE] Computed conditional probabilities in 0.019s\n",
      "[t-SNE] Iteration 50: error = 94.3613205, gradient norm = 0.2903698 (50 iterations in 0.591s)\n",
      "[t-SNE] Iteration 100: error = 109.2612000, gradient norm = 0.2391563 (50 iterations in 0.586s)\n",
      "[t-SNE] Iteration 150: error = 119.4839096, gradient norm = 0.2053123 (50 iterations in 0.555s)\n",
      "[t-SNE] Iteration 200: error = 124.2377319, gradient norm = 0.1871849 (50 iterations in 0.499s)\n",
      "[t-SNE] Iteration 250: error = 128.3783417, gradient norm = 0.1860656 (50 iterations in 0.491s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 128.378342\n",
      "[t-SNE] Iteration 300: error = 3.6556814, gradient norm = 0.0005521 (50 iterations in 0.520s)\n",
      "[t-SNE] Iteration 350: error = 3.1418393, gradient norm = 0.0002943 (50 iterations in 0.546s)\n",
      "[t-SNE] Iteration 400: error = 2.8691053, gradient norm = 0.0001251 (50 iterations in 0.525s)\n",
      "[t-SNE] Iteration 450: error = 2.6916778, gradient norm = 0.0000831 (50 iterations in 0.522s)\n",
      "[t-SNE] Iteration 500: error = 2.5715985, gradient norm = 0.0000621 (50 iterations in 0.550s)\n",
      "[t-SNE] Iteration 550: error = 2.4750330, gradient norm = 0.0000514 (50 iterations in 0.561s)\n",
      "[t-SNE] Iteration 600: error = 2.3927426, gradient norm = 0.0000407 (50 iterations in 0.546s)\n",
      "[t-SNE] Iteration 650: error = 2.3277524, gradient norm = 0.0000352 (50 iterations in 0.559s)\n",
      "[t-SNE] Iteration 700: error = 2.2700222, gradient norm = 0.0000305 (50 iterations in 0.540s)\n",
      "[t-SNE] Iteration 750: error = 2.2215879, gradient norm = 0.0000262 (50 iterations in 0.546s)\n",
      "[t-SNE] KL divergence after 750 iterations: 2.221588\n"
     ]
    }
   ],
   "source": [
    "my_tsne_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_scaled)\n",
    "\n",
    "my_tsne_bedrock_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_bedrock_scaled)\n",
    "my_tsne_superficial_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "my_tsne_test = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the t-SNE dimensions for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_train = pd.DataFrame(data = my_tsne_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_bedrock_train = pd.DataFrame(data = my_tsne_bedrock_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_superficial_train = pd.DataFrame(data = my_tsne_superficial_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_test = pd.DataFrame(data = my_tsne_test, columns = ['tsne1', 'tsne2', 'tsne3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored for the purpose of two-dimensional and three-dimensional visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'PC_df_train' (DataFrame)\n",
      "Stored 'my_pca_train' (PCA)\n",
      "Stored 'PC_df_bedrock_train' (DataFrame)\n",
      "Stored 'my_pca_train_bedrock' (PCA)\n",
      "Stored 'PC_df_superficial_train' (DataFrame)\n",
      "Stored 'my_pca_train_superficial' (PCA)\n",
      "Stored 'PC_df_test' (DataFrame)\n",
      "Stored 'my_pca_test' (PCA)\n",
      "Stored 'tsne_df_train' (DataFrame)\n",
      "Stored 'tsne_df_bedrock_train' (DataFrame)\n",
      "Stored 'tsne_df_superficial_train' (DataFrame)\n",
      "Stored 'tsne_df_test' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store PC_df_train\n",
    "%store my_pca_train\n",
    "\n",
    "%store PC_df_bedrock_train\n",
    "%store my_pca_train_bedrock\n",
    "\n",
    "%store PC_df_superficial_train\n",
    "%store my_pca_train_superficial\n",
    "\n",
    "%store PC_df_test\n",
    "%store my_pca_test\n",
    "\n",
    "%store tsne_df_train\n",
    "\n",
    "%store tsne_df_bedrock_train\n",
    "%store tsne_df_superficial_train\n",
    "\n",
    "\n",
    "%store tsne_df_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
