{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preproccessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configurations\n",
    "* classify_bedrock_only -> True|False, if set to True then classes for classification are bedrock sites only\n",
    "* group_superficial -> True|False, if set to True then some superficial sites are grouped together to raise class-specific F1 scores for the associated sites\n",
    "* drop_fake_bedrock ->  True|False, if set to True then some bedrock sites deemed not to be true bedrock sites are not used for classification\n",
    "* export_dimensions ->  True|False, if set to True then exports t-SNE and PCA dimensions as .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_path = 'data/raw_data.csv'\n",
    "\n",
    "group_superficial = True\n",
    "drop_fake_bedrock = True\n",
    "classify_bedrock_only = False\n",
    "export_dimensions = False\n",
    "reduced_sites = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(data_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FH', 'ER', 'WW', 'TC', 'CS', 'BC', 'KQ', 'AR', 'SL', 'FG', 'WB',\n",
       "       'BX', 'PF', 'BM', 'WH', 'SQ', 'BP', 'WN', 'BH', 'PH', 'LB', 'AB',\n",
       "       'LV', 'BR', 'KY', 'BF', 'ST', 'SH', 'CF', 'BG', 'AC', 'CR', 'GH',\n",
       "       'PX', 'WF', 'DH', 'NMAG_Gold', 'NMW_Gold', 'NMWGwern', 'UBSS',\n",
       "       'Cefn', 'Stockley', 'Pucha', 'Woodbury', 'Pimple', 'Wellington',\n",
       "       'Lyonshall', 'SymondsYatE', 'Madawg', nan], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['Site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_classes_grouped(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        elif row['Site'] == 'BC' or row['Site'] == 'CS':\n",
    "            return('BC_CS')\n",
    "        elif row['Site'] == 'SQ' or row['Site'] == 'BP':\n",
    "            return('SQ_BP')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "        \n",
    "def make_classes_grouped_reduced(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "\n",
    "def make_classes_raw(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        return(row['Region'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['class'] = 'init'   \n",
    "\n",
    "\n",
    "if drop_fake_bedrock and not group_superficial:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "    \n",
    "if group_superficial and not drop_fake_bedrock:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "else:\n",
    "    my_data['class'] = my_data.apply(make_classes_raw, axis = 1)\n",
    "if reduced_sites == True:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped_reduced, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_fake_bedrock:\n",
    "    my_data = my_data[(my_data['class'] != 'BM') & (my_data['class'] != 'BC') & (my_data['class'] != 'BP') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove '<' signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 25260.42it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24108.41it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24422.63it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22702.91it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23401.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24156.45it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 25229.09it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24018.48it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 20937.39it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22727.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22767.23it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22501.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23410.12it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23332.09it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23207.24it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23783.64it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23883.12it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23585.79it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24971.94it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23945.15it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24176.91it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 25300.48it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23669.75it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22963.92it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24071.68it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23312.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23649.71it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24415.95it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23727.56it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23499.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23571.70it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24336.71it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24082.01it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23134.57it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22773.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23752.32it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24109.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24677.99it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24316.17it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24016.71it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23317.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23588.69it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23335.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23613.85it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 22606.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23720.72it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24052.29it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 24368.69it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23926.75it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23036.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23702.15it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23857.30it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 23700.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    def fill_less_than(row):\n",
    "        if 'DL' in  str(row[column_name]):\n",
    "            return(np.nan)\n",
    "        if '<' in str(row[column_name]):\n",
    "            return(float(row[column_name].replace('<', '').replace(',', '')))\n",
    "        else:\n",
    "            return(float(row[column_name]))\n",
    "    my_data[column_name] = my_data.swifter.apply(fill_less_than, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_data = my_data.dropna(thresh = 15 , axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute na values with variable mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    my_data[column_name] = my_data[column_name].fillna(my_data[column_name].mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers defined as any values that exceed 2 standard deviations from the mean, such values are changed to the mean for that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dict = {}\n",
    "mean_dict = {}\n",
    "median_dict = {}\n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    std_dict[col] = my_data[col].std()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    mean_dict[col] = my_data[col].mean()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    median_dict[col] = my_data[col].median()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29528.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28322.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27801.66it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28256.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27915.78it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29107.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27814.27it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28079.07it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28974.32it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29539.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28252.29it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29279.51it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29688.14it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27335.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28595.99it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29324.06it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29227.19it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27547.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27167.89it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27690.52it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29306.27it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28228.91it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28590.35it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28313.98it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29989.73it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 30662.30it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29823.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28265.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27707.96it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29485.07it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27031.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27813.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29153.87it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27248.89it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27409.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29079.98it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28835.63it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 30035.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29621.29it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29517.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27607.24it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 30251.97it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 30968.77it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28602.01it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28901.20it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 30032.37it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29368.74it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29660.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29923.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 29424.55it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 27582.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 28433.23it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 30767.29it/s]\n"
     ]
    }
   ],
   "source": [
    "for col_name in my_data.columns.values[9:-1]:\n",
    "    def impute_outliers(row):\n",
    "        if np.abs(row[col_name] - mean_dict[col_name]) > 2*(std_dict[col_name]):\n",
    "            return(mean_dict[col_name])\n",
    "        else:\n",
    "            return(row[col_name])\n",
    "    my_data[col_name]= my_data.swifter.apply(impute_outliers, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for samples for which the source is known for training the model and those for which the source is not known (artefacts) for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = my_data[(my_data['Geology']== 'Bedrock') | (my_data['Geology'] == 'Superficial')]\n",
    "test_data = my_data[my_data['Geology']=='Artefacts']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encode the class to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_formodel = train_data.copy(deep = True)\n",
    "if classify_bedrock_only == False: \n",
    "    train_data_formodel['class'], uniques = pd.factorize(train_data_formodel['class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_formodel' (DataFrame)\n",
      "Stored 'train_data' (DataFrame)\n",
      "Stored 'test_data' (DataFrame)\n",
      "Stored 'my_data' (DataFrame)\n",
      "Stored 'uniques' (Index)\n"
     ]
    }
   ],
   "source": [
    "%store train_data_formodel\n",
    "%store train_data\n",
    "%store test_data\n",
    "%store my_data\n",
    "%store uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data is split into two datasets, one superficial and one bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bedrock = train_data[train_data['Geology'] == 'Bedrock']\n",
    "train_data_superficial = train_data[train_data['Geology'] == 'Superficial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the columns containimg the mass spectrometry data are stored as variables, 4 datsets are stored, one containing all train data (bedrock and superficial), one containing just bedrock, one containing just superficial and one containing the artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232', 'U238'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_bedrock.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train = train_data[train_data.columns.values[9:-1]]\n",
    "element_data_train_bedrock = train_data_bedrock[train_data.columns.values[9:-1]]\n",
    "element_data_train_superficial = train_data_superficial[train_data.columns.values[9:-1]]\n",
    "element_data_test = test_data[test_data.columns.values[9:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I scale the mass spectrometry data to have a mean 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler_train = StandardScaler()\n",
    "my_scaler_train_bedrock = StandardScaler()\n",
    "my_scaler_train_superficial = StandardScaler()\n",
    "my_scaler_test = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train_scaled = my_scaler_train.fit_transform(element_data_train)\n",
    "element_data_train_bedrock_scaled = my_scaler_train_bedrock.fit_transform(element_data_train_bedrock)\n",
    "element_data_train_superficial_scaled = my_scaler_train_superficial.fit_transform(element_data_train_superficial)\n",
    "element_data_test_scaled = my_scaler_test.fit_transform(element_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA is utilised on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca_train = PCA(n_components=element_data_train_scaled.shape[1])\n",
    "\n",
    "my_pca_train_bedrock = PCA(n_components=element_data_train_bedrock_scaled.shape[1])\n",
    "my_pca_train_superficial = PCA(n_components=element_data_train_superficial_scaled.shape[1])\n",
    "\n",
    "my_pca_test = PCA(n_components=element_data_test_scaled.shape[1])\n",
    "\n",
    "element_data_train_pca = my_pca_train.fit_transform(element_data_train_scaled)\n",
    "\n",
    "element_data_train_bedrock_pca = my_pca_train_bedrock.fit_transform(element_data_train_bedrock_scaled)\n",
    "element_data_train_superficial_pca = my_pca_train_superficial.fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "element_data_test_pca = my_pca_test.fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the principal components for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_PCs = element_data_train_scaled.shape[1]\n",
    "PC_names = []\n",
    "for i in range(0, no_PCs):\n",
    "    number = i + 1\n",
    "    column_name = 'PC' + str(number)\n",
    "    PC_names.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_df_train = pd.DataFrame(data = element_data_train_pca, columns = PC_names)\n",
    "\n",
    "PC_df_bedrock_train = pd.DataFrame(data = element_data_train_bedrock_pca, columns = PC_names)\n",
    "PC_df_superficial_train = pd.DataFrame(data = element_data_train_superficial_pca, columns = PC_names)\n",
    "\n",
    "PC_df_test = pd.DataFrame(data = element_data_test_pca, columns = PC_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE is utilised on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 1158 samples in 0.002s...\n",
      "[t-SNE] Computed neighbors for 1158 samples in 0.185s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1158\n",
      "[t-SNE] Computed conditional probabilities for sample 1158 / 1158\n",
      "[t-SNE] Mean sigma: 2.027810\n",
      "[t-SNE] Computed conditional probabilities in 0.059s\n",
      "[t-SNE] Iteration 50: error = 72.3841400, gradient norm = 0.1013953 (50 iterations in 5.721s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-198e72bdb81a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_tsne_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_data_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_tsne_bedrock_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_data_train_bedrock_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_tsne_superficial_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_data_train_superficial_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    856\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \"\"\"\n\u001b[0;32m--> 858\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    768\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 812\u001b[0;31m                                                       **opt_args)\n\u001b[0m\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m             print(\"[t-SNE] KL divergence after %d iterations with early \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose)\u001b[0m\n\u001b[1;32m    245\u001b[0m     error = _barnes_hut_tsne.gradient(val_P, X_embedded, neighbors, indptr,\n\u001b[1;32m    246\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m                                       dof=degrees_of_freedom)\n\u001b[0m\u001b[1;32m    248\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_tsne_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_scaled)\n",
    "\n",
    "my_tsne_bedrock_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_bedrock_scaled)\n",
    "my_tsne_superficial_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "my_tsne_test = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the t-SNE dimensions for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_train = pd.DataFrame(data = my_tsne_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_bedrock_train = pd.DataFrame(data = my_tsne_bedrock_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_superficial_train = pd.DataFrame(data = my_tsne_superficial_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_test = pd.DataFrame(data = my_tsne_test, columns = ['tsne1', 'tsne2', 'tsne3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored for the purpose of two-dimensional and three-dimensional visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store PC_df_train\n",
    "%store my_pca_train\n",
    "\n",
    "%store PC_df_bedrock_train\n",
    "%store my_pca_train_bedrock\n",
    "\n",
    "%store PC_df_superficial_train\n",
    "%store my_pca_train_superficial\n",
    "\n",
    "%store PC_df_test\n",
    "%store my_pca_test\n",
    "\n",
    "%store tsne_df_train\n",
    "\n",
    "%store tsne_df_bedrock_train\n",
    "%store tsne_df_superficial_train\n",
    "\n",
    "\n",
    "%store tsne_df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The t-SNE dimensions and principal components are stored as .csv files to be potentially used alongside other features for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_dimensions:\n",
    "    %store -r tsne_df_train\n",
    "    tsne_df_train.to_csv('tsne_df_train.csv', index = False)\n",
    "    %store -r tsne_df_test\n",
    "    tsne_df_test.to_csv('tsne_df_test.csv', index=False)\n",
    "\n",
    "    %store -r PC_df_train\n",
    "    PC_df_train.to_csv(' PC_df_train.csv', index=False)\n",
    "    %store -r PC_df_test\n",
    "    PC_df_test.to_csv('PC_df_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
