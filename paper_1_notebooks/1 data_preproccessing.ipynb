{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preproccessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configurations\n",
    "* classify_bedrock_only -> True|False, if set to True then classes for classification are bedrock sites only\n",
    "* group_superficial -> True|False, if set to True then some superficial sites are grouped together to raise class-specific F1 scores for the associated sites\n",
    "* drop_fake_bedrock ->  True|False, if set to True then some bedrock sites deemed not to be true bedrock sites are not used for classification\n",
    "* export_dimensions ->  True|False, if set to True then exports t-SNE and PCA dimensions as .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_path = '../data/raw_data.csv'\n",
    "\n",
    "group_superficial = True\n",
    "drop_fake_bedrock = True\n",
    "classify_bedrock_only = False\n",
    "export_dimensions = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(data_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FH', 'ER', 'WW', 'TC', 'CS', 'BC', 'KQ', 'AR', 'SL', 'FG', 'WB',\n",
       "       'BX', 'PF', 'BM', 'WH', 'SQ', 'BP', 'WN', 'BH', 'PH', 'LB', 'AB',\n",
       "       'LV', 'BR', 'KY', 'BF', 'ST', 'SH', 'CF', 'BG', 'AC', 'CR', 'GH',\n",
       "       'PX', 'WF', 'DH', 'NMAG_Gold', 'NMW_Gold', 'NMWGwern', 'UBSS',\n",
       "       'Cefn', 'Stockley', 'Pucha', 'Woodbury', 'Pimple', 'Wellington',\n",
       "       'Lyonshall', 'SymondsYatE', 'Madawg', nan], dtype=object)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['Site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_classes_grouped(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        elif row['Site'] == 'BC' or row['Site'] == 'CS':\n",
    "            return('BC_CS')\n",
    "        elif row['Site'] == 'SQ' or row['Site'] == 'BP':\n",
    "            return('SQ_BP')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "        \n",
    "def make_classes_grouped_reduced(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "\n",
    "def make_classes_raw(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        return(row['Region'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['class'] = 'init'   \n",
    "\n",
    "\n",
    "if drop_fake_bedrock and not group_superficial:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "    \n",
    "if group_superficial and not drop_fake_bedrock:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "else:\n",
    "    my_data['class'] = my_data.apply(make_classes_raw, axis = 1)\n",
    "if reduced_sites == True:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped_reduced, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_fake_bedrock:\n",
    "    my_data = my_data[(my_data['class'] != 'BM') & (my_data['class'] != 'BC') & (my_data['class'] != 'BP') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove '<' signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 17861.72it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 12539.17it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14717.59it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16878.66it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15963.22it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16728.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16375.76it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14370.24it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16625.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 12262.24it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16673.82it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14326.56it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14523.59it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15968.97it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 13420.28it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15427.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16519.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 11801.00it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14324.48it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15321.98it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16773.24it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16897.81it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 13818.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14876.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14353.38it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15635.95it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15276.07it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 13855.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 13375.94it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16742.95it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14122.88it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16149.31it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15534.79it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16453.32it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16856.63it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15347.31it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 10495.89it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 11402.44it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 12042.16it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15370.32it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14277.51it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15359.55it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14543.45it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16073.26it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16984.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 13670.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15522.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16188.34it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14721.88it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14498.55it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 16099.66it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 14045.81it/s]\n",
      "Pandas Apply: 100%|██████████| 1552/1552 [00:00<00:00, 15412.75it/s]\n"
     ]
    }
   ],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    def fill_less_than(row):\n",
    "        if 'DL' in  str(row[column_name]):\n",
    "            return(np.nan)\n",
    "        if '<' in str(row[column_name]):\n",
    "            return(float(row[column_name].replace('<', '').replace(',', '')))\n",
    "        else:\n",
    "            return(float(row[column_name]))\n",
    "    my_data[column_name] = my_data.swifter.apply(fill_less_than, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_data = my_data.dropna(thresh = 15 , axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute na values with variable mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    my_data[column_name] = my_data[column_name].fillna(my_data[column_name].mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers defined as any values that exceed 2 standard deviations from the mean, such values are changed to the mean for that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dict = {}\n",
    "mean_dict = {}\n",
    "median_dict = {}\n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    std_dict[col] = my_data[col].std()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    mean_dict[col] = my_data[col].mean()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    median_dict[col] = my_data[col].median()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 19115.35it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18835.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 19430.55it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 13897.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15678.66it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 12818.63it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17663.53it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 16903.64it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17346.07it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 14427.24it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15204.03it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17269.95it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17435.10it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 13616.74it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 14354.68it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 16135.28it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18448.26it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18315.58it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 13145.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17484.22it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 16011.81it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17642.72it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17594.06it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18870.07it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17839.02it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15680.32it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18294.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17451.69it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15003.51it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18094.74it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18337.64it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18658.42it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15948.64it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17363.77it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 13939.77it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15997.91it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15511.00it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17786.89it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17351.40it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 16168.61it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 15069.36it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 16583.75it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 14364.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17708.78it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18640.53it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18406.58it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17446.92it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 14613.36it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 19074.25it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 16364.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 18221.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17064.88it/s]\n",
      "Pandas Apply: 100%|██████████| 1521/1521 [00:00<00:00, 17023.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for col_name in my_data.columns.values[9:-1]:\n",
    "    def impute_outliers(row):\n",
    "        if np.abs(row[col_name] - mean_dict[col_name]) > 2*(std_dict[col_name]):\n",
    "            return(mean_dict[col_name])\n",
    "        else:\n",
    "            return(row[col_name])\n",
    "    my_data[col_name]= my_data.swifter.apply(impute_outliers, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for samples for which the source is known for training the model and those for which the source is not known (artefacts) for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = my_data[(my_data['Geology']== 'Bedrock') | (my_data['Geology'] == 'Superficial')]\n",
    "test_data = my_data[my_data['Geology']=='Artefacts']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encode the class to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_formodel = train_data.copy(deep = True)\n",
    "if classify_bedrock_only == False: \n",
    "    train_data_formodel['class'], uniques = pd.factorize(train_data_formodel['class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_formodel' (DataFrame)\n",
      "Stored 'train_data' (DataFrame)\n",
      "Stored 'test_data' (DataFrame)\n",
      "Stored 'my_data' (DataFrame)\n",
      "Stored 'uniques' (Index)\n"
     ]
    }
   ],
   "source": [
    "%store train_data_formodel\n",
    "%store train_data\n",
    "%store test_data\n",
    "%store my_data\n",
    "%store uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data is split into two datasets, one superficial and one bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bedrock = train_data[train_data['Geology'] == 'Bedrock']\n",
    "train_data_superficial = train_data[train_data['Geology'] == 'Superficial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the columns containimg the mass spectrometry data are stored as variables, 4 datsets are stored, one containing all train data (bedrock and superficial), one containing just bedrock, one containing just superficial and one containing the artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bedrock.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train = train_data[train_data.columns.values[9:-1]]\n",
    "element_data_train_bedrock = train_data_bedrock[train_data.columns.values[9:-1]]\n",
    "element_data_train_superficial = train_data_superficial[train_data.columns.values[9:-1]]\n",
    "element_data_test = test_data[test_data.columns.values[9:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I scale the mass spectrometry data to have a mean 0 and standard deviation of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler_train = StandardScaler()\n",
    "my_scaler_train_bedrock = StandardScaler()\n",
    "my_scaler_train_superficial = StandardScaler()\n",
    "my_scaler_test = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train_scaled = my_scaler_train.fit_transform(element_data_train)\n",
    "element_data_train_bedrock_scaled = my_scaler_train_bedrock.fit_transform(element_data_train_bedrock)\n",
    "element_data_train_superficial_scaled = my_scaler_train_superficial.fit_transform(element_data_train_superficial)\n",
    "element_data_test_scaled = my_scaler_test.fit_transform(element_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA is utilised on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca_train = PCA(n_components=element_data_train_scaled.shape[1])\n",
    "\n",
    "my_pca_train_bedrock = PCA(n_components=element_data_train_bedrock_scaled.shape[1])\n",
    "my_pca_train_superficial = PCA(n_components=element_data_train_superficial_scaled.shape[1])\n",
    "\n",
    "my_pca_test = PCA(n_components=element_data_test_scaled.shape[1])\n",
    "\n",
    "element_data_train_pca = my_pca_train.fit_transform(element_data_train_scaled)\n",
    "\n",
    "element_data_train_bedrock_pca = my_pca_train_bedrock.fit_transform(element_data_train_bedrock_scaled)\n",
    "element_data_train_superficial_pca = my_pca_train_superficial.fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "element_data_test_pca = my_pca_test.fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the principal components for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_PCs = element_data_train_scaled.shape[1]\n",
    "PC_names = []\n",
    "for i in range(0, no_PCs):\n",
    "    number = i + 1\n",
    "    column_name = 'PC' + str(number)\n",
    "    PC_names.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_df_train = pd.DataFrame(data = element_data_train_pca, columns = PC_names)\n",
    "\n",
    "PC_df_bedrock_train = pd.DataFrame(data = element_data_train_bedrock_pca, columns = PC_names)\n",
    "PC_df_superficial_train = pd.DataFrame(data = element_data_train_superficial_pca, columns = PC_names)\n",
    "\n",
    "PC_df_test = pd.DataFrame(data = element_data_test_pca, columns = PC_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE is utilised on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tsne_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_scaled)\n",
    "\n",
    "my_tsne_bedrock_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_bedrock_scaled)\n",
    "my_tsne_superficial_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "my_tsne_test = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the t-SNE dimensions for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_train = pd.DataFrame(data = my_tsne_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_bedrock_train = pd.DataFrame(data = my_tsne_bedrock_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_superficial_train = pd.DataFrame(data = my_tsne_superficial_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_test = pd.DataFrame(data = my_tsne_test, columns = ['tsne1', 'tsne2', 'tsne3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored for the purpose of two-dimensional and three-dimensional visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store PC_df_train\n",
    "%store my_pca_train\n",
    "\n",
    "%store PC_df_bedrock_train\n",
    "%store my_pca_train_bedrock\n",
    "\n",
    "%store PC_df_superficial_train\n",
    "%store my_pca_train_superficial\n",
    "\n",
    "%store PC_df_test\n",
    "%store my_pca_test\n",
    "\n",
    "%store tsne_df_train\n",
    "\n",
    "%store tsne_df_bedrock_train\n",
    "%store tsne_df_superficial_train\n",
    "\n",
    "\n",
    "%store tsne_df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The t-SNE dimensions and principal components are stored as .csv files to be potentially used alongside other features for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if export_dimensions:\n",
    "    %store -r tsne_df_train\n",
    "    tsne_df_train.to_csv('tsne_df_train.csv', index = False)\n",
    "    %store -r tsne_df_test\n",
    "    tsne_df_test.to_csv('tsne_df_test.csv', index=False)\n",
    "\n",
    "    %store -r PC_df_train\n",
    "    PC_df_train.to_csv(' PC_df_train.csv', index=False)\n",
    "    %store -r PC_df_test\n",
    "    PC_df_test.to_csv('PC_df_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
