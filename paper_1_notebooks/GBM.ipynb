{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot\n",
    "import pickle\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load variables stored by data_preproccessing notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data_formodel\n",
    "%store -r test_data\n",
    "%store -r my_data\n",
    "%store -r uniques\n",
    "%store -r best_feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configurations\n",
    "* save_plots -> True|False\n",
    "* random_seed_state -> number, sets random state for model and for stratified splits \n",
    "* classify_bedrock_only -> True|False\n",
    "* pickle_model -> True|False, wether model should be serialised and saved\n",
    "* pickle_model_name -> string, name of serialised model\n",
    "* grid_search -> True|False, if set to true then grid search is performed to identify optimum hyperparamaters for model \n",
    "* scale -> True|False if set to True then features scaled to all have mean value 0 and standard deviation 1\n",
    "* pickle_file_path -> string,  filepath for serialised model to be saved to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_plots = False\n",
    "random_seed_state = 42\n",
    "classify_bedrock_only = False\n",
    "pickle_model = False\n",
    "pickle_model_name = 'grouped'\n",
    "pickle_file_path = '../../../model'\n",
    "grid_search = False\n",
    "scale = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### if only bedrock sites are classified then classes are label encoded, if bedrock sites alone are not being classified then the class sites would have already been label encoded in the 1 data_preproccessing notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if classify_bedrock_only:\n",
    "    train_data_formodel['class'], uniques = pd.factorize(train_data_formodel['class'])\n",
    "    train_data_formodel = train_data_formodel[train_data_formodel['Geology']=='Bedrock']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### counts of instances in all classes before oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22    120\n",
       "4     105\n",
       "23    105\n",
       "16    100\n",
       "21     74\n",
       "17     61\n",
       "24     60\n",
       "0      53\n",
       "12     45\n",
       "14     36\n",
       "2      36\n",
       "15     36\n",
       "6      30\n",
       "11     30\n",
       "10     30\n",
       "7      30\n",
       "20     28\n",
       "5      27\n",
       "8      27\n",
       "19     27\n",
       "1      24\n",
       "13     21\n",
       "3      18\n",
       "18     18\n",
       "9      17\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_formodel['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The class column is stored as the variable y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(train_data_formodel['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The variables identified as best by the 2 feature_selection notebook are used as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_feats = train_data_formodel[best_feats]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### address class imbalance using synthetic minority oversampling technique (SMOTE) algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale:\n",
    "    my_scaler = StandardScaler()\n",
    "    X = np.array(my_scaler.fit_transform(np.array(train_data_feats)))\n",
    "else:\n",
    "    X = np.array(np.array(train_data_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the dimensions of the class and features are checked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 31)\n",
      "(3000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search is done to identify best hyperparamaters for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if grid_search:\n",
    "    esti = xgb.XGBClassifier(booster = 'gbtree')\n",
    "\n",
    "\n",
    "    \n",
    "    n_estimators = [int(x) for x in np.linspace(start = 4000, stop = 8000, num = 4)]\n",
    "    max_depth = [int(x) for x in np.linspace(10, 60, num = 3)]\n",
    "    min_child_weight = [0,3]\n",
    "    gamma = [0,3]\n",
    "\n",
    "    \n",
    "    \n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "                   'min_child_weight':min_child_weight,\n",
    "                   'gamma':gamma\n",
    "                  }\n",
    "    \n",
    "    clf = RandomizedSearchCV(estimator = esti, n_iter=20, param_distributions = random_grid,\n",
    "                              n_jobs=-1, scoring='f1_macro', cv = 3, verbose=3, \n",
    "                              random_state = random_seed_state)\n",
    "    \n",
    "    clf.fit(X, y)\n",
    "    print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best paramaters  from the randomised grid search were {'n_estimators': 6666, 'min_child_weight': 3, 'max_depth': 10, 'gamma': 0}, took 149 mins to fit 60 models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if grid_search:\n",
    "    esti = xgb.XGBClassifier(booster = 'gbtree')\n",
    "\n",
    "    n_estimators = [5000, 6000, 7000]\n",
    "    min_child_weight = [3, 5]\n",
    "    max_depth = [5,10]\n",
    "\n",
    "    \n",
    "    \n",
    "    random_grid = {'n_estimators': n_estimators,\n",
    "               'min_child_weight': min_child_weight,\n",
    "                  'max_depth':max_depth\n",
    "                  }\n",
    "    \n",
    "    clf = GridSearchCV(estimator = esti, param_grid = random_grid,\n",
    "                              n_jobs=-1, scoring='f1_macro', cv = 3, verbose=3 \n",
    "                              )\n",
    "    clf.fit(X, y)\n",
    "    print(clf.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best paramaters from the grid search were {'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 5000}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carry out 10-f0ld stratified cross validation, class f1 scores and macro f1 scores with weighted averages are calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making model:\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmorse/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making model:\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rmorse/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making model:\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4f1849e802ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mesti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mesti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mclass_f1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set)\u001b[0m\n\u001b[1;32m    545\u001b[0m                               \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m                               verbose_eval=verbose, xgb_model=None)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"objective\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m-> 1021\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1022\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "esti = xgb.XGBClassifier(booster=\"gbtree\", max_depth = 5, min_child_weight=3, n_estimators=5000)\n",
    "skf = StratifiedKFold(n_splits=10, random_state=random_seed_state)\n",
    "skf.get_n_splits(X, y)\n",
    "class_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "accuracy_scores = []\n",
    "feat_imp =[]\n",
    "f1_dict = {}\n",
    "feat_imp_dict = {}\n",
    "count = 0\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    count = count + 1\n",
    "    print('making model:')\n",
    "    key = 'round' + str(count)\n",
    "    print(count)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    X_post_smote, y_post_smote = SMOTE(random_state=42).fit_sample(X_train, y_train)\n",
    "        \n",
    "    esti.fit(X_train, y_train)\n",
    "    y_pred = esti.predict(X_test)\n",
    "    class_f1_scores = f1_score(y_test, y_pred, average = None)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    macro_f1_scores.append(f1_score(y_test, y_pred, average = 'weighted'))\n",
    "    f1_dict[key] = class_f1_scores \n",
    "    feat_imp_dict[key] = esti.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = pd.DataFrame(data = f1_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f1_dict:\n",
    "    print(len(f1_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the encodings for the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_formodel['class'].unique())\n",
    "print(list(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final = pd.concat([f1_df, pd.Series(uniques)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final.rename(columns={0:'class'}, inplace=True)\n",
    "f1_df_final.set_index('class', drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the distribution of class f1 scores from 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plot = sns.boxplot(data = f1_df_final.T)\n",
    "plot.set_title('F1 scores for each site', fontdict={'fontsize': 14})\n",
    "plot.set_ylabel('F1 score', fontdict={'fontsize': 11})\n",
    "plot.set_xlabel(\"Bedrock site or superficial site\", fontdict={'fontsize': 11})\n",
    "\n",
    "if save_plots == True:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('output/site_specific_f1_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(data = f1_df_final.T.median()).to_csv('output/median_class_f1_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the macro F1 score with weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.boxplot(macro_f1_scores)\n",
    "plot.set_title('Average-weighted macro-f1 score', fontdict={'fontsize': 14})\n",
    "plot.set_xlabel(\"F1-score\", fontdict={'fontsize': 11})\n",
    "\n",
    "if save_plots == True:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('output/macro_f1_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.Series(pd.Series(macro_f1_scores).median()).to_csv('output/median_macro_f1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(macro_f1_scores).median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat_imp_df = pd.DataFrame(data = feat_imp_dict)\n",
    "feat_imp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat_imp_df_final = pd.concat([feat_imp_df, pd.Series(my_data[my_data.columns.values[9:-1]].columns.values)], axis = 1)\n",
    "feat_imp_df_final.rename(columns = {0:'element'}, inplace = True )\n",
    "feat_imp_df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat_imp_df_final.set_index('element', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat_imp_df_final_plot = feat_imp_df_final.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feat_imp_df_final_plot\n",
    "\n",
    "elements = feat_imp_df_final_plot.columns.values \n",
    "mean_feature_importance = []\n",
    "for col in list(feat_imp_df_final_plot.columns.values):\n",
    "    mean_feature_importance.append(feat_imp_df_final_plot[col].mean())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_feature_importance_df = pd.concat([pd.Series(elements), pd.Series(mean_feature_importance)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_feature_importance_df.rename(columns={0:'elements', 1:'mean_importance'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_feature_importance_df.sort_values(by='mean_importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ordered_col_names = list(mean_feature_importance_df['elements'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_style()\n",
    "sns.set(rc={'figure.figsize':(20,20)})\n",
    "plot = sns.boxplot(data = feat_imp_df_final_plot[ordered_col_names])\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation=90, ha = 'left')\n",
    "plot.set_title('Feature (element) importance', fontdict={'fontsize': 20})\n",
    "plot.set_ylabel('Feature importance', fontdict={'fontsize': 15})\n",
    "plot.set_xlabel(\"Element\", fontdict={'fontsize': 15})\n",
    "\n",
    "if save_plots == True:\n",
    "    fig = plot.get_figure()\n",
    "    fig.savefig('output/feature_importances.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model is built for predicting source of artefacts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esti_final = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esti_final.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pickle_model == True:\n",
    "    pickle.dump(esti_final, open(pickle_file_path+'.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(train_data_formodel['class'].unique())\n",
    "print(uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_for_identifiers = test_data.copy(deep = True)\n",
    "identifiers =  df_for_identifiers['Analysis']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions are made for the artefacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred = esti_final.predict(np.array(test_data[test_data.columns.values[9:-1]]))\n",
    "\n",
    "y_pred_proba = esti_final.predict_proba(np.array(test_data[test_data.columns.values[9:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probabilities_df = pd.DataFrame(data = y_pred_proba, columns = uniques)\n",
    "probabilities_df_final = pd.concat([probabilities_df, pd.Series(list(identifiers))], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probabilities_df_final.rename(columns = {0:'identifier'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_pred_df = pd.concat([pd.Series(y_pred), probabilities_df_final], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_pred_df.rename(columns={0:'class_number'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions are outputted as csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_pred_df.to_csv('output/predictions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uniques_list = list(uniques)\n",
    "def get_pred_names(row):\n",
    "    return(uniques_list[row['class_number']])\n",
    "final_pred_df['class_predictions'] = final_pred_df.apply(get_pred_names, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "final_pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
