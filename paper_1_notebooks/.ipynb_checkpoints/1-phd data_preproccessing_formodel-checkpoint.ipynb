{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preproccessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot\n",
    "\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_path = '../data/raw_data.csv'\n",
    "classify_bedrock_only = False\n",
    "group_superficial = True\n",
    "reduced_sites = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(data_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FH', 'ER', 'WW', 'TC', 'CS', 'BC', 'KQ', 'AR', 'SL', 'FG', 'WB',\n",
       "       'BX', 'PF', 'BM', 'WH', 'SQ', 'BP', 'WN', 'BH', 'PH', 'LB', 'AB',\n",
       "       'LV', 'BR', 'KY', 'BF', 'ST', 'SH', 'CF', 'BG', 'AC', 'CR', 'GH',\n",
       "       'PX', 'WF', 'DH', 'NMAG_Gold', 'NMW_Gold', 'NMWGwern', 'UBSS',\n",
       "       'Cefn', 'Stockley', 'Pucha', 'Woodbury', 'Pimple', 'Wellington',\n",
       "       'Lyonshall', 'SymondsYatE', 'Madawg', nan], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['Site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make labels for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (<ipython-input-17-a703f7ce5e5c>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-a703f7ce5e5c>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    continue\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def make_classes_grouped(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        elif row['Site'] == 'BC' or row['Site'] == 'CS':\n",
    "            return('BC_CS')\n",
    "        elif row['Site'] == 'SQ' or row['Site'] == 'BP':\n",
    "            return('SQ_BP')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "        \n",
    "def make_classes_grouped_reduced(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'BP':\n",
    "            continue\n",
    "        if row['Site'] == 'BC':\n",
    "            continue\n",
    "        if row['Site'] == 'BM':\n",
    "            continue\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "\n",
    "def make_classes_raw(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        return(row['Region'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['class'] = 'init'   \n",
    "\n",
    "if reduced_sites and group_superficial:\n",
    "    data['class'] = my_data.apply(make_classes_grouped_reduced, axis = 1)\n",
    "    \n",
    "if group_superficial and not group_superficial:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "else:\n",
    "    my_data['class'] = my_data.apply(make_classes_raw, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove '<' signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    def fill_less_than(row):\n",
    "        if 'DL' in  str(row[column_name]):\n",
    "            return(np.nan)\n",
    "        if '<' in str(row[column_name]):\n",
    "            return(float(row[column_name].replace('<', '').replace(',', '')))\n",
    "        else:\n",
    "            return(float(row[column_name]))\n",
    "    my_data[column_name] = my_data.swifter.apply(fill_less_than, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_data = my_data.dropna(thresh = 15 , axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute na values with variable median, this is more resistant to the effect of outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    my_data[column_name] = my_data[column_name].fillna(my_data[column_name].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dict = {}\n",
    "mean_dict = {}\n",
    "median_dict = {}\n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    std_dict[col] = my_data[col].std()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    mean_dict[col] = my_data[col].mean()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    median_dict[col] = my_data[col].median()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col_name in my_data.columns.values[9:-1]:\n",
    "    def impute_outliers(row):\n",
    "        if np.abs(row[col_name] - mean_dict[col_name]) > 2*(std_dict[col_name]):\n",
    "            return(mean_dict[col_name])\n",
    "        else:\n",
    "            return(row[col_name])\n",
    "    my_data[col_name]= my_data.swifter.apply(impute_outliers, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter for known data bedrock data for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = my_data[(my_data['Geology']== 'Bedrock') | (my_data['Geology'] == 'Superficial')]\n",
    "test_data = my_data[my_data['Geology']=='Artefacts']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encode the class to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_formodel = train_data.copy(deep = True)\n",
    "if classify_bedrock_only == False: \n",
    "    train_data_formodel['class'], uniques = pd.factorize(train_data_formodel['class'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### order of class labels as numbers 0 through 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store train_data_formodel\n",
    "%store train_data\n",
    "%store test_data\n",
    "%store my_data\n",
    "%store uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data_bedrock = train_data[train_data['Geology'] == 'Bedrock']\n",
    "train_data_superficial = train_data[train_data['Geology'] == 'Superficial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_data_bedrock.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "element_data_train = train_data[train_data.columns.values[9:-1]]\n",
    "element_data_train_bedrock = train_data_bedrock[train_data.columns.values[9:-1]]\n",
    "element_data_train_superficial = train_data_superficial[train_data.columns.values[9:-1]]\n",
    "element_data_test = test_data[test_data.columns.values[9:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_scaler_train = StandardScaler()\n",
    "my_scaler_train_bedrock = StandardScaler()\n",
    "my_scaler_train_superficial = StandardScaler()\n",
    "my_scaler_test = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "element_data_train_scaled = my_scaler_train.fit_transform(element_data_train)\n",
    "element_data_train_bedrock_scaled = my_scaler_train_bedrock.fit_transform(element_data_train_bedrock)\n",
    "element_data_train_superficial_scaled = my_scaler_train_superficial.fit_transform(element_data_train_superficial)\n",
    "element_data_test_scaled = my_scaler_test.fit_transform(element_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_pca_train = PCA(n_components=element_data_train_scaled.shape[1])\n",
    "\n",
    "my_pca_train_bedrock = PCA(n_components=element_data_train_bedrock_scaled.shape[1])\n",
    "my_pca_train_superficial = PCA(n_components=element_data_train_superficial_scaled.shape[1])\n",
    "\n",
    "my_pca_test = PCA(n_components=element_data_test_scaled.shape[1])\n",
    "\n",
    "element_data_train_pca = my_pca_train.fit_transform(element_data_train_scaled)\n",
    "\n",
    "element_data_train_bedrock_pca = my_pca_train_bedrock.fit_transform(element_data_train_bedrock_scaled)\n",
    "element_data_train_superficial_pca = my_pca_train_superficial.fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "element_data_test_pca = my_pca_test.fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no_PCs = element_data_train_scaled.shape[1]\n",
    "PC_names = []\n",
    "for i in range(0, no_PCs):\n",
    "    number = i + 1\n",
    "    column_name = 'PC' + str(number)\n",
    "    PC_names.append(column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC_df_train = pd.DataFrame(data = element_data_train_pca, columns = PC_names)\n",
    "\n",
    "PC_df_bedrock_train = pd.DataFrame(data = element_data_train_bedrock_pca, columns = PC_names)\n",
    "PC_df_superficial_train = pd.DataFrame(data = element_data_train_superficial_pca, columns = PC_names)\n",
    "\n",
    "PC_df_test = pd.DataFrame(data = element_data_test_pca, columns = PC_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my_tsne_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_scaled)\n",
    "\n",
    "my_tsne_bedrock_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_bedrock_scaled)\n",
    "my_tsne_superficial_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_superficial_scaled)\n",
    "\n",
    "my_tsne_test = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tsne_df_train = pd.DataFrame(data = my_tsne_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_bedrock_train = pd.DataFrame(data = my_tsne_bedrock_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_superficial_train = pd.DataFrame(data = my_tsne_superficial_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "\n",
    "tsne_df_test = pd.DataFrame(data = my_tsne_test, columns = ['tsne1', 'tsne2', 'tsne3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%store PC_df_train\n",
    "%store my_pca_train\n",
    "\n",
    "%store PC_df_bedrock_train\n",
    "%store my_pca_train_bedrock\n",
    "\n",
    "%store PC_df_superficial_train\n",
    "%store my_pca_train_superficial\n",
    "\n",
    "%store PC_df_test\n",
    "%store my_pca_test\n",
    "\n",
    "%store tsne_df_train\n",
    "\n",
    "%store tsne_df_bedrock_train\n",
    "%store tsne_df_superficial_train\n",
    "\n",
    "\n",
    "%store tsne_df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "%store -r tsne_df_train\n",
    "tsne_df_train.to_csv('tsne_df_train.csv', index = False)\n",
    "%store -r tsne_df_test\n",
    "tsne_df_test.to_csv('tsne_df_test.csv', index=False)\n",
    "\n",
    "%store -r PC_df_train\n",
    "PC_df_train.to_csv(' PC_df_train.csv', index=False)\n",
    "%store -r PC_df_test\n",
    "PC_df_test.to_csv('PC_df_test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
