{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preproccessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import modules and configure notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configurations\n",
    "* data_input_path -> string, filepath to data to be read in\n",
    "\n",
    "* grouping -> boolean, if set to True them many sites are grouped\n",
    "* reduced_grouping -> boolean, if set to True then less sites grouped, only bedrock sites 'WB' and 'BX are grouped into one class and superficial sites 'SV' and 'SE' are grouped into one class.\n",
    "* raw -> boolean, if set to True then no grouping done\n",
    "* drop_semi_bedrock ->  True|False, if set to True then some bedrock sites deemed to be semi-bedrock sites are not used for classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_path = '../data/raw_data.csv'\n",
    "\n",
    "\n",
    "grouping = True\n",
    "reduced_grouping = False\n",
    "raw = False\n",
    "drop_semi_bedrock = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(data_input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.columns.values[9:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw sample names including sample sites and artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FH', 'ER', 'WW', 'TC', 'CS', 'BC', 'KQ', 'AR', 'SL', 'FG', 'WB',\n",
       "       'BX', 'PF', 'BM', 'WH', 'SQ', 'BP', 'WN', 'BH', 'PH', 'LB', 'AB',\n",
       "       'LV', 'BR', 'KY', 'BF', 'ST', 'SH', 'CF', 'BG', 'AC', 'CR', 'GH',\n",
       "       'PX', 'WF', 'DH', 'NMAG_Gold', 'NMW_Gold', 'NMWGwern', 'UBSS',\n",
       "       'Cefn', 'Stockley', 'Pucha', 'Woodbury', 'Pimple', 'Wellington',\n",
       "       'Lyonshall', 'SymondsYatE', 'Madawg', nan], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data['Site'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define functions for making target classes for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_classes_grouped(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        elif row['Site'] == 'BC' or row['Site'] == 'CS':\n",
    "            return('BC_CS')\n",
    "        elif row['Site'] == 'SQ' or row['Site'] == 'BP':\n",
    "            return('SQ_BP')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "        \n",
    "def make_classes_grouped_reduced(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        if row['Site'] == 'WB' or row['Site'] == 'BX':\n",
    "            return('WB_BX')\n",
    "        else:\n",
    "            return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        if row['Region'] == 'SV' or row['Region'] == 'SE':\n",
    "            return('SV_SE')\n",
    "        else:\n",
    "            return(row['Region'])\n",
    "\n",
    "def make_classes_raw(row):\n",
    "    if row['Geology'] == 'Bedrock':\n",
    "        return(row['Site'])\n",
    "    elif row['Geology'] == 'Superficial':\n",
    "        return(row['Region'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### targets for classification are made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data['class'] = 'init'   \n",
    "\n",
    "if grouping:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped, axis = 1)\n",
    "elif grouping_reduced:\n",
    "    my_data['class'] = my_data.apply(make_classes_grouped_reduced, axis = 1)\n",
    "elif raw:\n",
    "    my_data['class'] = my_data.apply(make_classes_raw, axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if drop_semi_bedrock:\n",
    "    my_data = my_data[(my_data['class'] != 'BM') & (my_data['class'] != 'BC') & (my_data['class'] != 'BP') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove '<' signs and commas from feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 24421.41it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23475.23it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 19523.97it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 19549.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 17754.57it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23119.20it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 20153.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21617.82it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23744.93it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22391.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23002.57it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23164.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22408.35it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22222.57it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23082.55it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23007.39it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23572.38it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22457.11it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21715.66it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22529.16it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22334.60it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22886.10it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23220.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21137.96it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22088.75it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 18639.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21562.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22414.50it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 17613.72it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21001.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21762.29it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22516.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 20674.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 20925.99it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22587.51it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22275.70it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23377.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21380.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 20957.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21818.44it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22775.60it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23236.38it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21083.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23172.79it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23984.62it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23214.39it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 23583.33it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21776.87it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 24012.99it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22416.22it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 22354.43it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21123.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1597/1597 [00:00<00:00, 21641.64it/s]\n"
     ]
    }
   ],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    def fill_less_than(row):\n",
    "        if 'DL' in  str(row[column_name]):\n",
    "            return(np.nan)\n",
    "        if '<' in str(row[column_name]):\n",
    "            return(float(row[column_name].replace('<', '').replace(',', '')))\n",
    "        else:\n",
    "            return(float(row[column_name]))\n",
    "    my_data[column_name] = my_data.swifter.apply(fill_less_than, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove rows where there are all na values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = my_data.dropna(how = 'all' , axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute na values with feature mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column_name in my_data.columns.values[9:-1]:\n",
    "    my_data[column_name] = my_data[column_name].fillna(my_data[column_name].mean()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers defined as any values that exceed 2 standard deviations from the mean, such values are changed to the mean for that variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dict = {}\n",
    "mean_dict = {}\n",
    "median_dict = {}\n",
    "\n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    std_dict[col] = my_data[col].std()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    mean_dict[col] = my_data[col].mean()\n",
    "    \n",
    "for col in my_data.columns.values[9:-1]:\n",
    "    median_dict[col] = my_data[col].median()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 24871.41it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27367.56it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 28049.83it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27941.46it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27938.98it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 28740.73it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26337.12it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27320.16it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27353.74it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27460.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26728.49it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25490.90it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 24940.00it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 21204.28it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26783.18it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25339.44it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 24639.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25749.27it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25166.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 22952.56it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25034.08it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25352.36it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27735.47it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26260.54it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 23457.81it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26613.78it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26681.34it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25370.17it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26149.38it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26327.79it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 22435.60it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 23171.65it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26027.57it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26974.67it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26480.83it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26697.19it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25914.92it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26286.75it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26793.39it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25158.80it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27038.00it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27065.45it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26687.38it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 27083.42it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26050.76it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 24890.79it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 23321.83it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 24776.04it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 25947.88it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 23612.16it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26248.44it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 26106.73it/s]\n",
      "Pandas Apply: 100%|██████████| 1576/1576 [00:00<00:00, 28386.26it/s]\n"
     ]
    }
   ],
   "source": [
    "for col_name in my_data.columns.values[9:-1]:\n",
    "    def impute_outliers(row):\n",
    "        if np.abs(row[col_name] - mean_dict[col_name]) > 2*(std_dict[col_name]):\n",
    "            return(mean_dict[col_name])\n",
    "        else:\n",
    "            return(row[col_name])\n",
    "    my_data[col_name]= my_data.swifter.apply(impute_outliers, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split data into 'train_data' and 'test_data', the former consists of samples from known geological sites and the latter from flint artefacts fow which the original geological source site is unknown and to be predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = my_data[(my_data['Geology']== 'Bedrock') | (my_data['Geology'] == 'Superficial')]\n",
    "test_data = my_data[my_data['Geology']=='Artefacts']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### label encode the class to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_formodel = train_data.copy(deep = True)\n",
    "train_data_formodel['class'], uniques = pd.factorize(train_data_formodel['class'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'train_data_formodel' (DataFrame)\n",
      "Stored 'train_data' (DataFrame)\n",
      "Stored 'test_data' (DataFrame)\n",
      "Stored 'my_data' (DataFrame)\n",
      "Stored 'uniques' (Index)\n"
     ]
    }
   ],
   "source": [
    "%store train_data_formodel\n",
    "%store train_data\n",
    "%store test_data\n",
    "%store my_data\n",
    "%store uniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preproccessing for dimensionality reduction and visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'training_data' is split into two datasets, one consisting of samples from superficial sites and one containing samples from bedrock sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_bedrock = train_data[train_data['Geology'] == 'Bedrock']\n",
    "train_data_superficial = train_data[train_data['Geology'] == 'Superficial']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### four datasets are created, one containing all train data (bedrock and superficial types), one containing just bedrock types, one containing just superficial types and one containing the artefacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train = train_data[train_data.columns.values[9:-1]]\n",
    "element_data_train_bedrock = train_data_bedrock[train_data.columns.values[9:-1]]\n",
    "element_data_train_superficial = train_data_superficial[train_data.columns.values[9:-1]]\n",
    "element_data_test = test_data[test_data.columns.values[9:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### features are standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler_train = StandardScaler()\n",
    "my_scaler_train_bedrock = StandardScaler()\n",
    "my_scaler_train_superficial = StandardScaler()\n",
    "my_scaler_test = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_data_train_scaled = my_scaler_train.fit_transform(element_data_train)\n",
    "element_data_train_bedrock_scaled = my_scaler_train_bedrock.fit_transform(element_data_train_bedrock)\n",
    "element_data_train_superficial_scaled = my_scaler_train_superficial.fit_transform(element_data_train_superficial)\n",
    "element_data_test_scaled = my_scaler_test.fit_transform(element_data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The four datasets are transformed using Principal component analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_pca_train = PCA(n_components=element_data_train_scaled.shape[1])\n",
    "my_pca_train_bedrock = PCA(n_components=element_data_train_bedrock_scaled.shape[1])\n",
    "my_pca_train_superficial = PCA(n_components=element_data_train_superficial_scaled.shape[1])\n",
    "my_pca_test = PCA(n_components=element_data_test_scaled.shape[1])\n",
    "\n",
    "element_data_train_pca = my_pca_train.fit_transform(element_data_train_scaled)\n",
    "element_data_train_bedrock_pca = my_pca_train_bedrock.fit_transform(element_data_train_bedrock_scaled)\n",
    "element_data_train_superficial_pca = my_pca_train_superficial.fit_transform(element_data_train_superficial_scaled)\n",
    "element_data_test_pca = my_pca_test.fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the principal components for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_PCs = element_data_train_scaled.shape[1]\n",
    "PC_names = []\n",
    "for i in range(0, no_PCs):\n",
    "    number = i + 1\n",
    "    column_name = 'PC' + str(number)\n",
    "    PC_names.append(column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_df_train = pd.DataFrame(data = element_data_train_pca, columns = PC_names)\n",
    "PC_df_bedrock_train = pd.DataFrame(data = element_data_train_bedrock_pca, columns = PC_names)\n",
    "PC_df_superficial_train = pd.DataFrame(data = element_data_train_superficial_pca, columns = PC_names)\n",
    "PC_df_test = pd.DataFrame(data = element_data_test_pca, columns = PC_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T-Distributed Stochastic Neighbour Embedding is done on the four datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 1213 samples in 0.007s...\n",
      "[t-SNE] Computed neighbors for 1213 samples in 0.218s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 1213\n",
      "[t-SNE] Computed conditional probabilities for sample 1213 / 1213\n",
      "[t-SNE] Mean sigma: 2.025622\n",
      "[t-SNE] Computed conditional probabilities in 0.081s\n",
      "[t-SNE] Iteration 50: error = 72.2213898, gradient norm = 0.0896249 (50 iterations in 4.079s)\n",
      "[t-SNE] Iteration 100: error = 72.4473495, gradient norm = 0.0898499 (50 iterations in 2.787s)\n",
      "[t-SNE] Iteration 150: error = 72.5215302, gradient norm = 0.0823327 (50 iterations in 2.427s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-754e07888fcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_tsne_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_data_train_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmy_tsne_bedrock_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_data_train_bedrock_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmy_tsne_superficial_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_data_train_superficial_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmy_tsne_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m750\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_data_test_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mEmbedding\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdimensional\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \"\"\"\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m    804\u001b[0m                           \u001b[0mX_embedded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m                           \u001b[0mneighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneighbors_nn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                           skip_num_points=skip_num_points)\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0mP\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_exaggeration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         params, kl_divergence, it = _gradient_descent(obj_func, params,\n\u001b[0;32m--> 848\u001b[0;31m                                                       **opt_args)\n\u001b[0m\u001b[1;32m    849\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             print(\"[t-SNE] KL divergence after %d iterations with early \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, n_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compute_error'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mn_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0mgrad_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/manifold/t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error)\u001b[0m\n\u001b[1;32m    259\u001b[0m                                       \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                                       \u001b[0mdof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m                                       compute_error=compute_error)\n\u001b[0m\u001b[1;32m    262\u001b[0m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_tsne_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_scaled)\n",
    "my_tsne_bedrock_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_bedrock_scaled)\n",
    "my_tsne_superficial_train = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_train_superficial_scaled)\n",
    "my_tsne_test = TSNE(n_components=3, n_iter=750, verbose=3).fit_transform(element_data_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the t-SNE dimensions for the four datasets are put into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_df_train = pd.DataFrame(data = my_tsne_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_bedrock_train = pd.DataFrame(data = my_tsne_bedrock_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_superficial_train = pd.DataFrame(data = my_tsne_superficial_train, columns = ['tsne1', 'tsne2', 'tsne3'])\n",
    "tsne_df_test = pd.DataFrame(data = my_tsne_test, columns = ['tsne1', 'tsne2', 'tsne3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### datasets are stored for the purpose of two-dimensional visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store PC_df_train\n",
    "%store my_pca_train\n",
    "\n",
    "%store PC_df_bedrock_train\n",
    "%store my_pca_train_bedrock\n",
    "\n",
    "%store PC_df_superficial_train\n",
    "%store my_pca_train_superficial\n",
    "\n",
    "%store PC_df_test\n",
    "%store my_pca_test\n",
    "\n",
    "%store tsne_df_train\n",
    "\n",
    "%store tsne_df_bedrock_train\n",
    "%store tsne_df_superficial_train\n",
    "\n",
    "\n",
    "%store tsne_df_test\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
