{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in our libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import swifter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot\n",
    "\n",
    "pd.set_option('max.rows', None)\n",
    "pd.set_option('max.columns', None)\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r train_data_formodel\n",
    "%store -r test_data\n",
    "%store -r my_data\n",
    "%store -r uniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data_formodel[train_data_formodel.columns.values[9:,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Li7', 'Be9', 'B11', 'Mg24', 'Al27', 'Si28', 'P31', 'S33', 'K39',\n",
       "       'Ca42', 'Sc45', 'Ti47', 'V51', 'Cr52', 'Mn55', 'Fe56', 'Co59',\n",
       "       'Ni60', 'Cu63', 'Zn68', 'Ga69', 'Ge72', 'As75', 'Rb85', 'Sr88',\n",
       "       'Y89', 'Zr90', 'Nb93', 'Mo95', 'Cd111', 'In115', 'Sn118', 'Cs133',\n",
       "       'Ba137', 'La139', 'Ce140', 'Pr141', 'Nd146', 'Sm147', 'Eu153',\n",
       "       'Gd157', 'Tb159', 'Dy163', 'Ho165', 'Er166', 'Tm169', 'Yb172',\n",
       "       'Lu175', 'Hf178', 'Ta181', 'Pb208', 'Th232', 'U238', 'class'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(train_data['class'])\n",
    "train_data_feats = train_data.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(train_data_feats)\n",
    "x_test = np.array(test_data[test_data.columns.values[9:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1243, 53)\n",
      "(363, 53)\n",
      "(1243,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful parameters which will come in handy later on\n",
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]\n",
    "SEED = 42 # for reproducibility\n",
    "NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, set_seed = True):\n",
    "\n",
    "        if set_seed:\n",
    "            params['random_state'] = seed\n",
    "\n",
    "            \n",
    "        self.clf = clf(**params)\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbgg_params = {\\n    'n_jobs' : -1,\\n    'n_estimators':50\\n}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 50, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 1\n",
    "}'''\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':50,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "# AdaBoost parameters\n",
    "ada_params = {\n",
    "    'n_estimators': 50,\n",
    "    'learning_rate' : 0.75\n",
    "}\n",
    "\n",
    "# Gradient Boosting parameters\n",
    "gb_params = {\n",
    "    'n_estimators': 50,\n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 5,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "svc_params = {\n",
    "    'C' : 0.025\n",
    "    }\n",
    "# Support Vector Classifier parameters \n",
    "\n",
    "\n",
    "knn_params = {\n",
    "    'n_jobs' : -1\n",
    "}\n",
    "'''\n",
    "bgg_params = {\n",
    "    'n_jobs' : -1,\n",
    "    'n_estimators':50\n",
    "}'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\n",
    "gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\n",
    "svc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n",
    "knn = SklearnHelper(clf=KNeighborsClassifier, params=knn_params, set_seed = False)\n",
    "#bgg = SklearnHelper(clf=BaggingClassifier, seed=SEED, params=bgg_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting random forest\n",
      "starting adaboost\n",
      "starting gradient boost\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1755.8990            9.53s\n",
      "         2        1410.0986            9.62s\n",
      "         3        1172.6790            9.56s\n",
      "         4         985.2995            9.51s\n",
      "         5         842.2294            9.40s\n",
      "         6         718.4228            9.24s\n",
      "         7         617.2412            9.07s\n",
      "         8         529.8198            8.89s\n",
      "         9         457.3486            8.68s\n",
      "        10         394.8410            8.50s\n",
      "        20         107.2611            6.41s\n",
      "        30          37.6446            4.27s\n",
      "        40          17.6137            2.13s\n",
      "        50          11.5389            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1717.6127            8.53s\n",
      "         2        1358.8969            9.19s\n",
      "         3        1114.5377            9.19s\n",
      "         4         933.8157            8.96s\n",
      "         5         788.9248            8.71s\n",
      "         6         669.9043            8.53s\n",
      "         7         575.2159            8.34s\n",
      "         8         497.0572            8.14s\n",
      "         9         430.1868            8.05s\n",
      "        10         372.0596            7.86s\n",
      "        20         103.7057            5.82s\n",
      "        30          40.2431            3.87s\n",
      "        40          22.7828            1.91s\n",
      "        50          17.2126            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1733.9556            9.34s\n",
      "         2        1384.2940            9.32s\n",
      "         3        1141.2257            9.19s\n",
      "         4         959.8253            9.02s\n",
      "         5         813.0443            8.91s\n",
      "         6         697.6718            8.72s\n",
      "         7         599.5370            8.55s\n",
      "         8         519.2476            8.37s\n",
      "         9         442.3725            8.24s\n",
      "        10         379.8557            8.11s\n",
      "        20         103.9982            6.26s\n",
      "        30          38.9932            4.25s\n",
      "        40          20.9057            2.13s\n",
      "        50          15.1950            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1658.6473           10.50s\n",
      "         2        1311.0868           10.43s\n",
      "         3        1075.0542           10.34s\n",
      "         4         897.7734           10.21s\n",
      "         5         749.0691           10.05s\n",
      "         6         631.3275            9.92s\n",
      "         7         534.1702            9.71s\n",
      "         8         451.2533            9.50s\n",
      "         9         383.5299            9.30s\n",
      "        10         324.7660            9.09s\n",
      "        20          78.8680            6.92s\n",
      "        30          26.5031            4.54s\n",
      "        40          13.9469            2.18s\n",
      "        50          10.7801            0.00s\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1        1675.7472           10.51s\n",
      "         2        1343.8452           10.54s\n",
      "         3        1107.7279           10.49s\n",
      "         4         932.7978           10.31s\n",
      "         5         788.5789           10.15s\n",
      "         6         670.1876            9.96s\n",
      "         7         568.2192            9.77s\n",
      "         8         484.1181            9.65s\n",
      "         9         415.2798            9.44s\n",
      "        10         354.9716            9.23s\n"
     ]
    }
   ],
   "source": [
    "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "print('starting random forest')\n",
    "#rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "print('starting adaboost')\n",
    "ada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost \n",
    "print('starting gradient boost')\n",
    "gb_oof_train, gb_oof_test = get_oof(gb,x_train, y_train, x_test) # Gradient Boost\n",
    "print('starting support vector machine')\n",
    "svc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\n",
    "print('starting knn')\n",
    "knn_oof_train, knn_oof_test = get_oof(knn,x_train, y_train, x_test)\n",
    "print('starting bagging')\n",
    "#bgg_oof_train, bgg_oof_test = get_oof(bgg,x_train, y_train, x_test)\n",
    "print(\"Training is complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train = pd.DataFrame( \n",
    "    {#'RandomForest': rf_oof_train.ravel(),\n",
    "    'ExtraTrees': et_oof_train.ravel(),\n",
    "     'AdaBoost': ada_oof_train.ravel(),\n",
    "      'GradientBoost': gb_oof_train.ravel(),\n",
    "    'supportvector': svc_oof_train.ravel(),\n",
    "    'knn': knn_oof_train.ravel()\n",
    "    #'bgg': bgg_oof_train.ravel()\n",
    "})\n",
    "base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_test = pd.DataFrame( \n",
    "    {#'RandomForest': rf_oof_test.ravel(),\n",
    "     'ExtraTrees': et_oof_test.ravel(),\n",
    "     'AdaBoost': ada_oof_test.ravel(),\n",
    "      'GradientBoost': gb_oof_test.ravel(),\n",
    "    'supportvector': svc_oof_test.ravel(),\n",
    "    'knn': knn_oof_test.ravel()\n",
    "   #  'bgg': bgg_oof_test.ravel()                                  \n",
    "    })\n",
    "base_predictions_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2_train = np.array(base_predictions_train)\n",
    "y_2_train = np.array(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictions_train.astype(float).corr().values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Heatmap(\n",
    "        z= base_predictions_train.astype(float).corr().values ,\n",
    "        x=base_predictions_train.columns.values,\n",
    "        y= base_predictions_train.columns.values,\n",
    "          colorscale='Rainbow',\n",
    "            showscale=True,\n",
    "            reversescale = True\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace]\n",
    "fig = dict(data=data)\n",
    "\n",
    "py.plot(fig, output_type='file', filename='corrheatmap'+ '.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = pd.DataFrame(data = base_predictions_train.astype(float).corr().values, columns = base_predictions_train.columns.values)\n",
    "corr_df_abs = corr_df.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summations = []\n",
    "for col in corr_df_abs.columns.values:\n",
    "    summations.append(corr_df_abs[col].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_correlation_summations = pd.DataFrame(data = b, index=list(a_df_abs.columns.values))\n",
    "model_correlation_summations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df_abs = a_df.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_df_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = xgb.XGBClassifier(max_depth = 20, n_estimators = 1000, n_jobs=-1)\n",
    "skf = StratifiedKFold(n_splits=10)\n",
    "skf.get_n_splits(X_2_train, y_2_train)\n",
    "class_f1_scores = []\n",
    "macro_f1_scores = []\n",
    "accuracy_scores = []\n",
    "feat_imp =[]\n",
    "f1_dict = {}\n",
    "count = 0\n",
    "for train_index, test_index in skf.split(X_2_train, y_2_train):\n",
    "    count = count + 1\n",
    "    print('making model:')\n",
    "    key = 'round' + str(count)\n",
    "    print(count)\n",
    "    X_train_loc, X_test_loc = X_2_train[train_index], X_2_train[test_index]\n",
    "    y_train_loc, y_test_loc = y_2_train[train_index], y_2_train[test_index]\n",
    "    xg.fit(X_train_loc, y_train_loc)\n",
    "    y_pred = xg.predict(X_test_loc)\n",
    "    class_f1_scores = f1_score(y_test_loc, y_pred, average = None)\n",
    "    accuracy = accuracy_score(y_test_loc, y_pred)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    macro_f1_scores.append(f1_score(y_test_loc, y_pred, average = 'weighted'))\n",
    "    f1_dict[key] = class_f1_scores \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df = pd.DataFrame(data = f1_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in f1_dict:\n",
    "    print(len(f1_dict[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below are the encodings for the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['class'].unique())\n",
    "print(list(uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final = pd.concat([f1_df, pd.Series(uniques)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_df_final.rename(columns={0:'class'}, inplace=True)\n",
    "f1_df_final.set_index('class', drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the distribution of class f1 scores from 10 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "plot = sns.boxplot(data = f1_df_final.T)\n",
    "plot.set_title('F1 scores for each site', fontdict={'fontsize': 14})\n",
    "plot.set_ylabel('F1 score', fontdict={'fontsize': 11})\n",
    "plot.set_xlabel(\"Bedrock site or superficial deposit 'region'\", fontdict={'fontsize': 11})\n",
    "\n",
    "#f save_plots == True:\n",
    "#    fig = plot.get_figure()\n",
    "#    fig.savefig('site_specific_f1_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing the macro F1 score with weighted averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = sns.boxplot(macro_f1_scores)\n",
    "plot.set_title('Average-weighted macro-f1 score', fontdict={'fontsize': 14})\n",
    "plot.set_xlabel(\"F1-score\", fontdict={'fontsize': 11})\n",
    "\n",
    "#if save_plots == True:\n",
    "#    fig = plot.get_figure()\n",
    "#    fig.savefig('macro_f1_scores.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot showing accuracy scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(accuracy_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
